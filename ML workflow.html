<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>ML workflow</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<h3 id="toc_0">ML Workflow</h3>

<h4 id="toc_1">Feature engineering - Label encoding and One-hot</h4>

<ul>
<li>First define the LabelEncoder</li>
<li>Second transfer it into every column using fit_transform(col)</li>
<li>check the after-converted data type to see if the non0numeric value has been converted into numeric value</li>
</ul>

<div><pre><code class="language-none">for column in non_numeric_columns:
    le = LabelEncoder()
    credit[column] = le.fit_transform(credit[column])
    
#Inspect the data types of the columns of the data frame
print(credit.dtypes)</code></pre></div>

<ul>
<li>label encoder just convert the diferent categories into different numberic number</li>
<li>One-hot encoding convert the different categories into different columns of dummy variable</li>
<li>use .fit_transform to apply the encoder to specific column</li>
</ul>

<div><pre><code class="language-none">
# Create numeric encoding for credit_history
credit_history_num = LabelEncoder().fit_transform(credit[&#39;credit_history&#39;])

# Create a new feature matrix including the numeric encoding
X_num = pd.concat([X, pd.Series(credit_history_num)], 1)

# Create new feature matrix with dummies for credit_history
X_hot = pd.concat(
  [X, pd.get_dummies(credit[&#39;credit_history&#39;])], 1)

# Compare the number of features of the resulting DataFrames
print(X_hot.shape[1] &gt; X_num.shape[1])</code></pre></div>

<h4 id="toc_2">Feature selection using SelectKBest()</h4>

<ul>
<li>we might need to transform some feature into a new features</li>
<li>then, we want to select the best feature that have close relationship with y</li>
<li>Score function:

<ul>
<li>For regression: f<em>regression, mutual_info</em>regression</li>
<li>For classification: chi2, f_classif, mutual_info_classif</li>
</ul></li>
</ul>

<div><pre><code class="language-none"># Function computing absolute difference from column mean
def abs_diff(x):
    return np.abs(x-np.mean(x))

# Apply it to the credit amount and store to new column
credit[&#39;diff&#39;] = abs_diff(credit[&#39;credit_amount&#39;])

# Create a feature selector with chi2 that picks one feature
sk = SelectKBest(chi2, k=1) 
#chi2 is the determine rule, k is the number feature we finally want to select.

# Use the selector to pick between credit_amount and diff
sk.fit(credit[[&#39;credit_amount&#39;, &#39;diff&#39;]], credit[&#39;class&#39;])
# the two arguments in the sk.fit is X(the candidate vairbales) and y

# Inspect the results
sk.get_support()</code></pre></div>

<div><pre><code class="language-none"># Find the best value for max_depth among values 2, 5 and 10
grid_search = GridSearchCV(
  rfc(random_state=1), param_grid={&#39;max_depth&#39;:[2,5,10]})
best_value = grid_search.fit(
  X_train, y_train).best_params_[&#39;max_depth&#39;]

# Using the best value from above, fit a random forest
clf = rfc(
  random_state=1, max_depth=best_value).fit(X_train, y_train)

# Apply SelectKBest with chi2 and pick top 100 features
vt = SelectKBest(chi2, k=100).fit(X_train, y_train)

# Create a new dataset only containing the selected features
X_train_reduced = vt.transform(X_train)</code></pre></div>

<h4 id="toc_3">Data Fusion</h4>

<ul>
<li>using groupby function with the function you created to reconstruct the </li>
</ul>

<div><pre><code class="language-none">#create a function first, to output the column you like

def featurize(df):
return {
&#39;unique_ports&#39;: len(set(df[&#39;destination_port&#39;])),
&#39;average_packet&#39;: np.mean(df[&#39;packet_count&#39;]),
&#39;average_duration&#39;: np.mean(df[&#39;duration&#39;])
}


#Group by source computer, and apply the feature extractor

out = flows.groupby(&#39;source_computer&#39;).apply(featurize)

#The output is a group-by dictionary

# Convert the iterator to a dataframe by calling list on it
X = pd.DataFrame(list(out), index=out.index)
#source_computer is the index because it is followed by groupby

# Check which sources in X.index are bad to create labels
y = [x in bads for x in X.index]

# Report the average accuracy of Adaboost over 3-fold CV
print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))</code></pre></div>

<h4 id="toc_4">Using lambda function and groupby to create the new df</h4>

<div><pre><code class="language-none"># Create a feature counting unique protocols per source
protocols = flows.groupby(&#39;source_computer&#39;).apply(
  lambda df: len(set(df[&#39;protocol&#39;])))
  
#using len(set(columns) to identify the number of unique value
#set() return the unique value of the list

# Convert this feature into a dataframe, naming the column
protocols_DF = pd.DataFrame(
  protocols, index=protocols.index, columns=[&#39;protocol&#39;])

# Now concatenate this feature with the previous dataset, X
X_more = pd.concat([X, protocols_DF], axis=1)

# Refit the classifier and report its accuracy
print(np.mean(cross_val_score(
  AdaBoostClassifier(), X_more, y)))</code></pre></div>

<h4 id="toc_5">Deal with imperfect label (some label might be wrong)</h4>

<ul>
<li>rebuild the label based on some shresholds of the vairbale </li>
<li>refit the model the re-evaluate the model after we modify the label</li>
</ul>

<div><pre><code class="language-none"># Create a new dataset X_train_bad by subselecting bad hosts
X_train_bad = X_train[y_train]

# Calculate the average of unique_ports in bad examples
avg_bad_ports = np.mean(X_train_bad[&#39;unique_ports&#39;])

# Label as positive sources that use more ports than that
pred_port = X_test[&#39;unique_ports&#39;] &gt; avg_bad_ports

# Print the accuracy of the heuristic
print(accuracy_score(y_test, pred_port))
</code></pre></div>

<ul>
<li>using two shresholds to recreate the label</li>
<li>use the principle： True*True = 1 to define the new label</li>
</ul>

<div><pre><code class="language-none"># Compute the mean of average_packet for bad sources
avg_bad_packet = np.mean(X_train[y_train][&#39;average_packet&#39;])

# Label as positive if average_packet is lower than that
pred_packet = X_test[&#39;average_packet&#39;] &lt; avg_bad_packet

# Find indices where pred_port and pred_packet both True
pred_port = X_test[&#39;unique_ports&#39;] &gt; avg_bad_ports
pred_both = pred_packet * pred_port #True*True=1, False*False=0, False*True = 0

# Ports only produced an accuracy of 0.919. Is this better?
print(accuracy_score(y_test, pred_both))</code></pre></div>

<h4 id="toc_6">Weighted Learning (need to be further learned)</h4>

<div><pre><code class="language-none"># Fit a Gaussian Naive Bayes classifier to the training data
clf = GaussianNB().fit(X_train, y_train_noisy)

# Report its accuracy on the test data
print(accuracy_score(y_test, clf.predict(X_test)))

# Assign half the weight to the first 100 noisy examples
weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)

# Refit using weights and report accuracy. Has it improved?
clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)
print(accuracy_score(y_test, clf_weights.predict(X_test)))</code></pre></div>

<h4 id="toc_7">Loss Function - Create new scoring method</h4>

<ul>
<li>Loss function is the rule to evaluate the model</li>
</ul>

<p>Split the tn, fp, fn and tp using confusion_matrix and ravel()
Then build the new loss function using tn fp fn and tp</p>

<div><pre><code class="language-none"># Get false positives/negatives from the confusion matrix
tn, fp,fn , tp = confusion_matrix(y_test, preds).ravel()

# Now compute the cost using the manager&#39;s advice
cost = fp*10 + fn*150    #That is a new loss function</code></pre></div>

<h4 id="toc_8">Adjust shreshold is also a method to adjust the loss_function</h4>

<ul>
<li>calculate the probability score of getting the target value</li>
<li>create the range of shreshold candidates</li>
<li>get the different predictive value based on the different shreshold value</li>
<li>get the different score of the different predictive value with the different shreshold, and select the best one with the best shreshold</li>
</ul>

<div><pre><code class="language-none"># Score the test data using the given classifier
scores = clf.predict_proba(X_test)

# Create a range of equally spaced threshold values
t_range = [ 0.0, 0.25, 0.5, 0.75, 1.0]

# Store the predicted labels for each value of the threshold
preds = [[s[1] &gt; thr for s in scores] for thr in t_range]

# Compute the accuracy for each threshold
accuracies = [accuracy_score(y_test, p) for p in preds]

# Compute the F1 score for each threshold
f1_scores = [f1_score(y_test, p) for p in preds]

# Report the optimal threshold for accuracy, and for F1
print(t_range[argmax(accuracies)], t_range[argmax(f1_scores)])</code></pre></div>

<h3 id="toc_9">Pipeline with different steps and different params of each steps</h3>

<ul>
<li>define a new scorer</li>
<li>create a pipeline including the several steps you want to process the data.

<ul>
<li>imputer: Imputer(missing<em>values=&quot;NaN&quot;,strategy=&#39;most</em>frequent&#39;, axis=0)</li>
<li>scaler: StandardScaler()</li>
<li>feature selection：  SelectKBest(f_classif)</li>
<li>model fitting</li>
</ul></li>
<li>create a dictionary of the parameters in different step</li>
<li>using __ to link the key name and step name</li>
<li>GirdSearch the pipe with the pipeline parameters and use new-defined scorer in the scoring arguemnt.</li>
<li>fit the pipeline using X<em>train and y</em>trian</li>
<li>using .best_params to get the best parameters of the model</li>
</ul>

<div><pre><code class="language-none"># Create a custom scorer
scorer = make_scorer(f1_score)

# Create pipeline with feature selector and classifier
pipe = Pipeline([
    (&#39;feature_selection&#39;, SelectKBest(f_classif)),
    (&#39;clf&#39;, RandomForestClassifier(random_state=2))])

# Create a parameter grid
params = {
   &#39;feature_selection__k&#39;:[10, 20],
    &#39;clf__n_estimators&#39;:[2, 5]}

# Initialize the grid search object
grid_search = GridSearchCV(pipe, param_grid=params, scoring = scorer)

# Fit it to the data and print the best value combination
print(grid_search.fit(X_train, y_train).best_params_)</code></pre></div>

<h4 id="toc_10">Deploy the model</h4>

<ul>
<li>you can create your own pipeline steps for yourself by defining a function</li>
<li>put your new function into the pipeline</li>
<li>define your pileline parameters</li>
<li>Girdsearch the pipeline with the defined parameters</li>
<li>fit the girdsearch to build the model</li>
<li>write the model into the file </li>
<li>load the model from the file when you want to use the model</li>
</ul>

<div><pre><code class="language-none"># Define a feature extractor to flag very large values (a new steps)

def more_than_average(X, multiplier=1.0):
  Z = X.copy()
  Z[:,1] = Z[:,1] &gt; multiplier*np.mean(Z[:,1])
  return Z

# Convert your function so that it can be used in a pipeline
pipe = Pipeline([
  (&#39;ft&#39;, FunctionTransformer(more_than_average)),
  (&#39;clf&#39;, RandomForestClassifier(random_state=2))])

# Optimize the parameter multiplier using GridSearchCV
params = {&#39;ft__multiplier&#39;:[1,2,3]}
grid_search = GridSearchCV(pipe, param_grid=params, cv = 3)
gs = grid_search.fit(X_train, y_train)

# Save it to a file, to be pushed to production
with open(&#39;model.pkl&#39;, &#39;wb&#39;) as file:   #write
    pickle.dump(gs, file=file)

# Now load the model from file in the production environment
with open(&#39;model.pkl&#39;,&quot;rb&quot;) as file:   #read
    gs_from_file = pickle.load(file)

# Predict the labels of the test dataset
preds = gs_from_file.predict(X_test)</code></pre></div>

<h3 id="toc_11">Detecting overtting</h3>

<ul>
<li>CV Training Score &gt;&gt;CV Test Score

<ul>
<li>overtting in model tting stage</li>
<li>reduce complexity of classier</li>
<li>get more training data</li>
<li>increase cv number</li>
</ul></li>
<li>CV Test Score &gt;&gt; Validation Score

<ul>
<li>overtting in model tuning stage</li>
<li>decrease cv number</li>
<li>decrease size of parameter grid</li>
</ul></li>
</ul>

<div><pre><code class="language-none"># Fit your pipeline using GridSearchCV with three folds
grid_search = GridSearchCV(
  pipe, params, cv=3, return_train_score=True)

# Fit the grid search
gs = grid_search.fit(X_train, y_train)

# Store the results of CV into a pandas dataframe
results = pd.DataFrame(gs.cv_results_)

# Print the difference between mean test and training scores
print(
  results[&#39;mean_test_score&#39;]-results[&#39;mean_train_score&#39;])</code></pre></div>

<h3 id="toc_12">Dataset shift</h3>

<ul>
<li>when the production data has slightly different difference with the model training dataset, then the pre-trained model will have a bad performance on the shifted data because the original model did not cpature the pattern of the new data.</li>
<li>reason of the dataset shift:

<ul>
<li>temporary change</li>
<li>domain shift: eg: the range of people that data was collected from is changed. </li>
</ul></li>
<li>What to do?

<ul>
<li>retrain the model using part of data using the Window size</li>
</ul></li>
</ul>

<div><pre><code class="language-none"># Loop over window sizes
for w_size in wrange:

    # Define sliding window
    sliding = arrh.loc[(t_now - w_size + 1):t_now]

    # Extract X and y from the sliding window
    X, y = sliding.drop(&#39;class&#39;, 1), sliding[&#39;class&#39;]
    
    # Fit the classifier and store the F1 score
    preds = GaussianNB().fit(X, y).predict(X_test)
    accuracies.append(f1_score(y_test, preds))

# Estimate the best performing window size
optimal_window = wrange[np.argmax(accuracies)]</code></pre></div>

<div><pre><code class="language-none"># Create a pipeline 
pipe = Pipeline([
  (&#39;ft&#39;, SelectKBest()), (&#39;clf&#39;, RandomForestClassifier(random_state=2))])

# Create a parameter grid
grid = {&#39;ft__k&#39;:[5, 10], &#39;clf__max_depth&#39;:[10, 20]}

# Execute grid search CV on a dataset containing under 50s
grid_search = GridSearchCV(pipe, param_grid=grid)
arrh = arrh.iloc[np.where(arrh[&#39;age&#39;] &lt; 50)]
grid_search.fit(arrh.drop(&#39;class&#39;, 1), arrh[&#39;class&#39;])

# Push the fitted pipeline to production
with open(&#39;pipe.pkl&#39;, &#39;wb&#39;) as file:
    pickle.dump(grid_search, file)
    </code></pre></div>

<h4 id="toc_13">Unsupervised ML</h4>

<ul>
<li>negative outlier factor: larger value means more normal</li>
<li>contamination: how many percent of the outlier you think in the dataset</li>
</ul>

<div><pre><code class="language-none"># Import the LocalOutlierFactor module
from sklearn.neighbors import LocalOutlierFactor as lof

# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained
x = [1]*30
x.append(10)

# Cast to a data frame
X = pd.DataFrame(x)

# Set the contamination parameter to 0.2
preds = lof(contamination=0.2).fit_predict(X)

# Print the confusion matrix
print(confusion_matrix(ground_truth,preds ))</code></pre></div>




</body>

</html>
